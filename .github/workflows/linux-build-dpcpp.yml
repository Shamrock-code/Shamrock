name: DPCPP


on:
  workflow_call:
    
jobs:
  nightly_build20230112:
    name: Cache Nightly build 2023-01-12
    runs-on: [self-hosted]
    
    steps:
        
      - name: Cache DPCPP
        id: cache-dpcpp
        uses: actions/cache@v3
        with:
          path: dpcpp_compiler
          key: dpcpp-nighly20230112.tar.gz
      
      - name : Download & Untar DPCPP
        if: steps.cache-dpcpp.outputs.cache-hit != 'true'
        run: |
          export DPCPP_URL="https://github.com/intel/llvm/releases/download/sycl-nightly%2F20230112/dpcpp-compiler.tar.gz"
          wget -q --show-progress --progress=bar:force:noscroll "$DPCPP_URL" -O dpcpp-compiler.tar.gz
          echo "untar : "
          tar -xf dpcpp-compiler.tar.gz
          echo "rm archive"
          rm dpcpp-compiler.tar.gz

  cuda_build:

    name: Cache CUDA ${{ matrix.cuda }}
    runs-on: [self-hosted]

    strategy:
      matrix:
        cuda: [11.0.2]
    
    steps:
        
      - name: Cache DPCPP
        id: cache-dpcpp
        uses: actions/cache@v3
        with:
          path: dpcpp_compiler
          key: dpcpp-cuda-${{ matrix.cuda }}.tar.gz

      - name: Git clone 
        if: steps.cache-dpcpp.outputs.cache-hit != 'true'
        run: |
          git clone https://github.com/intel/llvm -b sycl

      - name: install CUDA
        run: |
          mkdir -p ~/opt/cuda
          wget -q -O cuda.sh http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux.run
          sudo sh ./cuda.sh --override --silent --toolkit --no-man-page --no-drm --no-opengl-libs --installpath=~/opt/cuda || true
          echo "CUDA Version ${{matrix.cuda}}" | sudo tee ~/opt/cuda/version.txt
      
      - name: Configure
        if: steps.cache-dpcpp.outputs.cache-hit != 'true'
        run: |
          cd llvm
          CUDA_LIB_PATH=/home/docker/opt/cuda/lib64/stubs\
          python3 buildbot/configure.py \
            --llvm-external-projects compiler-rt \
            --cuda \
            --enable-esimd-emulator \
            --cmake-opt="-DCMAKE_INSTALL_PREFIX=../dpcpp_compiler" \
            --cmake-opt="-DCUDA_TOOLKIT_ROOT_DIR=/home/docker/opt/cuda"
        

    hip_build:

    name: Cache HIP ROCM ${{ matrix.rocm }}
    runs-on: [self-hosted]

    strategy:
      matrix:
        rocm: [5.4.3]
    
    steps:
        
      - name: Cache DPCPP
        id: cache-dpcpp
        uses: actions/cache@v3
        with:
          path: dpcpp_compiler
          key: dpcpp-hip-${{ matrix.rocm }}.tar.gz

      - name: Git clone 
        if: steps.cache-dpcpp.outputs.cache-hit != 'true'
        run: |
          git clone https://github.com/intel/llvm -b sycl

      - name: install ROCm
        run: |
          [[ ${{matrix.rocm}} == 4.0.1 ]] && CODENAME=xenial
          [[ ${{matrix.rocm}} == 5.4.3 ]] && CODENAME=focal
          sudo apt install -y libnuma-dev cmake unzip
          wget -q -O - https://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -
          echo "deb [arch=amd64] https://repo.radeon.com/rocm/apt/${{matrix.rocm}} $CODENAME main" | sudo tee /etc/apt/sources.list.d/rocm.list
          printf 'Package: *\nPin: release o=repo.radeon.com\nPin-Priority: 600' | sudo tee /etc/apt/preferences.d/rocm-pin-600
          sudo apt update -y
          sudo apt install -y rocm-dev
      
      - name: Configure
        if: steps.cache-dpcpp.outputs.cache-hit != 'true'
        run: |
          cd llvm
          CUDA_LIB_PATH=/home/docker/opt/cuda/lib64/stubs\
          python3 buildbot/configure.py \
            --llvm-external-projects compiler-rt \
            --hip \
            --enable-esimd-emulator \
            --cmake-opt="-DCMAKE_INSTALL_PREFIX=../dpcpp_compiler" \
            --cmake-opt="-DSYCL_BUILD_PI_HIP_ROCM_DIR=/usr/local/rocm"
        